{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABSTRACT\n",
    "\n",
    "This is the code related to the article presenting <b>the methodology IKE-XAI for Implicit Knowledge Extraction with eXplainable Artificial Intelligence</b> from the article. We use the <b>Tower of Hanoï (TOH)</b> as a guiding showcase of the proposed IKE-XAI methodology<br/> \n",
    "<b>[Chraibi Kaadoud et al, 2022]</b> \n",
    "CHRAIBI KAADOUD, Ikram , BENNETOT, Adrien, MAWHIN, Barbara, DIAZ-RODRIGUEZ, Natalia, Explaining  <i>Aha!</i> moments in artificial agents through IKE-XAI: Implicit Knowledge Extraction for eXplainable AI. <i>Neural Networks</i>, 2022,https://doi.org/10.1016/j.neunet.2022.08.002\n",
    "\n",
    "<p align = \"center\">\n",
    "<img src=\"Figures/ExperimentalDesign.png\" alt=\"drawing\" width=\"700\"/>\n",
    "</p>\n",
    "<p align = \"center\">\n",
    "    <b>Figure 2 of Chraibi Kaadoud et al (2022):</b> Experimental design of the IKE-XAI methodology to make explicit the process of the autonomous agent (AA) knowledge construction in three steps:<br/>\n",
    "<b>STEP 1) RL Phase:</b> a Q-learning agent learns to perform the TOH task. At several stages of the learning process,\n",
    "the training process is suspended to make a recording of the AA’s move sequences while it plays after learning. This step obtains: a) sequences of moves and b) an AA trained to perform TOH whose behavior is observable through its sequences of moves, to inform the solution chosen by the AA to reach the solution state (i.e., sequences of moves).<br/>\n",
    "<b>STEP 2) Moves Sequence Learning Phase:</b> the recorded sequences of moves of the AA are fed to train an LSTM\n",
    "to predict the AA’s next move at time t based on the current and past ones. This step returns a dataset of recordings of hidden patterns (i.e, the activity vectors of the hidden layer generated by the network at each input). The trained LSTM model had encoded an implicit representation of the TOH rules due to the learned sequences. Let us note that the trained LSTM model is trained on sequences generated from the TOH abstract representation (Figure B.13). <br/>\n",
    "<b>STEP 3) XAI Phase:</b> a post-hoc implicit rule extraction algorithm and a graph visualization technique are applied\n",
    "to the dataset of recorded hidden patterns to extract graphs of AA behavior at different stages of training.\n",
    "</p>\n",
    "\n",
    "\n",
    "The current notebook present the main steps of the IKE-XAI methodology. <br/>\n",
    "A breif presentation of the TOH task is provided in <b>[Chraibi Kaadoud et al, 2022] appendix B</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installation of requirements\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install networkx\n",
    "!pip install pydot\n",
    "!pip install tensorflow\n",
    "!pip install simplejson\n",
    "!pip install scikit-learn==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1 - Train the Q learning Artificial Agent (AA) on TOH task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RL Phase consists in training a Q-learning agent to perform the TOH task. At several stages of the learning process, the training process is suspended to make a recording of the AA’s move sequences while it plays after learning. <br/>\n",
    "This step obtains: <br/>\n",
    "<b>a)</b> sequences of moves <br/> \n",
    "<b>b)</b> an AA trained to perform TOH whose behavior is observable through its sequences of moves, to inform the solution chosen by the AA to reach the solution state (i.e., sequences of moves).<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import tools\n",
    "import os\n",
    "from os import path\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declaration of all functions for step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TowersOfHanoi:\n",
    "    def __init__(self, state):\n",
    "        # \"State\" is a tuple of length N, where N is the number of discs, and the elements are peg indices in [0,1,2]\n",
    "        self.state = state               \n",
    "        self.discs = len(self.state)\n",
    "\n",
    "    def discs_on_peg(self, peg):\n",
    "        return [disc for disc in range(self.discs) if self.state[disc] == peg]\n",
    "\n",
    "    def move_allowed(self, move):\n",
    "        discs_from = self.discs_on_peg(move[0])\n",
    "        discs_to = self.discs_on_peg(move[1])\n",
    "        if discs_from:\n",
    "            return (min(discs_to) > min(discs_from)) if discs_to else True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_moved_state(self, move):\n",
    "        if self.move_allowed(move):\n",
    "            disc_to_move = min(self.discs_on_peg(move[0]))\n",
    "        moved_state = list(self.state)\n",
    "        moved_state[disc_to_move] = move[1]\n",
    "        moved_state_t= tuple(moved_state)\n",
    "        return moved_state_t\n",
    "\n",
    "\n",
    "def generate_grammar_from_df(N, R,G,result_dir_for_N_disks, debug=False):\n",
    "    df_R = R.replace(0, 1)\n",
    "    df_R = df_R.replace(-np.inf, 0)\n",
    "    if debug:\n",
    "        print(\"type(df_R) : \", type(df_R))\n",
    "        print(\"df_R : \", df_R)\n",
    "        print(\" df_R column : \", df_R.columns)\n",
    "        print(\" df_R rows : \", df_R.index)\n",
    "\n",
    "    df_cols_tuple = df_R.columns.tolist()\n",
    "    df_cols = [str(x) for x in df_cols_tuple]\n",
    "\n",
    "    toh_grammar={}\n",
    "    for i in range(len(df_R.columns)):\n",
    "        node = str(df_R.columns[i])\n",
    "        if debug : print(\"\\n node : \", node)\n",
    "        list_transitions=[]\n",
    "        for one_neighbor in G.neighbors(node):\n",
    "            index_one_neighbor= list(df_cols).index(one_neighbor)\n",
    "            diff = \"\"\n",
    "            assert(len(node)==len(one_neighbor))\n",
    "            for t in range(len(node)):\n",
    "                if node[t] != one_neighbor[t]:\n",
    "                    diff = str(node[t]) + \"-\" + str(one_neighbor[t])\n",
    "            tuple=(diff, index_one_neighbor)\n",
    "            list_transitions.append(tuple)\n",
    "            if debug : print(\" =====>one_neighbor : \", one_neighbor, \" -  diff : \", diff)\n",
    "\n",
    "        toh_grammar[i]={\"transitions\": list_transitions, \"label\":node.replace(\" \",\"\").replace(\",\",\"\").replace(\"(\",\"\").replace(\")\",\"\")}\n",
    "\n",
    "    if debug : print (\"toh_grammar : \", toh_grammar)\n",
    "    res_file =  result_dir_for_N_disks+\"/TOH_\"+str(N)+\"_disks_grammar.json\"\n",
    "    tools.save_dict_in_json(toh_grammar,res_file)\n",
    "    print(\"TOH grammar with, \",str(N), \"disks saved in :\" ,res_file)\n",
    "    return toh_grammar\n",
    "\n",
    "def generate_graph_from_reward_matrix(N,R, result_dir_for_N_disks,debug=False):\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    from networkx.drawing.nx_agraph import write_dot\n",
    "\n",
    "    #1) transform the reward matrix into a numpy array matrices with 0 and 1 that represents the links between the states so that\n",
    "    #we can plot a graph from it to visualize the links\n",
    "    df_R =  R.replace(0, 1)\n",
    "    df_R = df_R.replace(-np.inf, 0)\n",
    "\n",
    "    if debug:\n",
    "        print (\"type(df_R) : \", type(df_R))\n",
    "        print (\"df_R : \", df_R)\n",
    "        print(\" df_R column : \", df_R.columns)\n",
    "        print(\" df_R rows : \", df_R.index)\n",
    "\n",
    "    G = nx.Graph()\n",
    "    labels = {}\n",
    "    for i in range(len(df_R.columns)):\n",
    "        a_col= str(df_R.columns[i])\n",
    "        labels[str(a_col)]= a_col\n",
    "\n",
    "\n",
    "    for i in range(len(df_R.columns)) :\n",
    "        if debug: print(\"i : \", i, \" -df_R.columns[i] :  \",df_R.columns[i])\n",
    "        for j in range(len(df_R.index)):\n",
    "            if debug :\n",
    "                print(\"j : \", j, \" -df_R.index[j] :  \",df_R.index[j])\n",
    "                print(\"df_R[df_R.columns[i]] : \", df_R[df_R.columns[i]])\n",
    "            cell_val = df_R.iat[i,j]\n",
    "            if cell_val>0:\n",
    "\n",
    "                src_node = str(df_R.columns[i])\n",
    "                target_node = str(df_R.index[j])\n",
    "                G.add_node(src_node, label=src_node)\n",
    "                G.add_node(target_node, label=target_node)\n",
    "                G.add_edge(src_node, target_node)\n",
    "\n",
    "    plt.figure(figsize=(15, 8))\n",
    "\n",
    "    nx.draw(G)\n",
    "    labels = nx.draw_networkx_labels(G, pos=nx.spring_layout(G))\n",
    "    plt.tight_layout()\n",
    "    plt.axis(\"off\")\n",
    "    res_file = result_dir_for_N_disks+\"/TOH_\"+str(N)+\"_disks_rules.png\"\n",
    "    plt.savefig(res_file)\n",
    "    print(\"TOH representation with \"+str(N)+\" disks saved : \", res_file)\n",
    "\n",
    "    if debug:\n",
    "        print(\"G.nodes(data=True)) : \", G.nodes(data=True))\n",
    "        print(\"G.nodes()) : \", G.nodes())\n",
    "        print(\"G.edges(data=True) : \", G.edges(data=True))\n",
    "        print (\"labels : \", labels)\n",
    "\n",
    "\n",
    "    return G\n",
    "\n",
    "# Generates the reward matrix for the Towers of Hanoi game as a Pandas DataFrame\n",
    "def generate_reward_matrix(N, debug=False):      # N is the number of discs\n",
    "\n",
    "    states = list(itertools.product(list(range(1,4)), repeat=N))\n",
    "    moves = list(itertools.permutations(list(range(1,4)), 2))\n",
    "    R = pd.DataFrame(index=states, columns=states, data=-np.inf)\n",
    "    for state in states:\n",
    "        tower = TowersOfHanoi(state=state)\n",
    "        for move in moves:\n",
    "            if tower.move_allowed(move):\n",
    "                next_state = tower.get_moved_state(move)\n",
    "                R[state][next_state] = 0\n",
    "    final_state = tuple([2]*N)          # Define final state as all discs being on the last peg\n",
    "    R[final_state] += 100               # Add a reward for all moves leading to the final state\n",
    "\n",
    "    if debug:\n",
    "        print(\"states : \", states)\n",
    "        print(\"moves : \", moves)\n",
    "        print(\"R all : \", R)\n",
    "\n",
    "    return R, R.values, states, moves\n",
    "\n",
    "# Learn the Q matrix\n",
    "def learn_Q(R, gamma=0.8, alpha=1.0, N_episodes=1000, random=False):\n",
    "    ##a learning rate α = 1, a discount factor γ = 0.8\n",
    "    Q = np.zeros(R.shape)\n",
    "    states=list(range(R.shape[0]))\n",
    "    for n in range(N_episodes):\n",
    "        Q_previous = Q\n",
    "        state = np.random.choice(states)                # Randomly select initial state\n",
    "        next_states = np.where(R[state,:] >= 0)[0]      # Generate a list of possible next states\n",
    "        next_state = np.random.choice(next_states)      # Randomly select next state from the list of possible next states\n",
    "        V = np.max(Q[next_state,:])                     # Maximum Q-value of the states accessible from the next state\n",
    "        if random==False:\n",
    "            Q[state, next_state] = (1-alpha)*Q[state, next_state] + alpha*(R[state, next_state] + gamma*V)      # Update Q-values\n",
    "    if np.max(Q) > 0:\n",
    "        Q /= np.max(Q)      # Normalize Q to its maximum value\n",
    "    return Q\n",
    "\n",
    "#Get the policy for each states\n",
    "def get_policy(Q, R):\n",
    "    Q_allowed = pd.DataFrame(Q)[pd.DataFrame(R) >= 0].values\n",
    "    \n",
    "    policy = []\n",
    "    for i in range(Q_allowed.shape[0]):\n",
    "        row = Q_allowed[i,:]\n",
    "        sorted_vals = np.sort(row)\n",
    "        sorted_vals = sorted_vals[~np.isnan(sorted_vals)][::-1]\n",
    "        sorted_args = row.argsort()[np.where(~np.isnan(sorted_vals))][::-1]\n",
    "        max_vals = [val for val in sorted_vals if val==sorted_vals[0]]\n",
    "        max_args = [sorted_args[i] for i,val in enumerate(sorted_vals) if val==sorted_vals[0]]\n",
    "        policy.append(max_args)\n",
    "        best_policy_value = max_vals[0]\n",
    "    return policy, best_policy_value\n",
    "\n",
    "#Play the defined policy\n",
    "def play(policy, allowed, epsilon=0, last_move=100, debug=False):\n",
    "\n",
    "    start_state = 0\n",
    "    end_state = len(policy)-1\n",
    "    state = start_state\n",
    "    moves = 0\n",
    "    number_asked = 0\n",
    "    ask_for_help = False\n",
    "\n",
    "    seq_states=[]\n",
    "    seq_states.append(state)\n",
    "    while state != end_state:\n",
    "        if last_move < 3:\n",
    "            test_eps = 100\n",
    "        else:\n",
    "            test_eps = random.uniform(0, 1)\n",
    "        \n",
    "        if test_eps < epsilon:\n",
    "            state = np.random.choice(allowed[state])\n",
    "        else:\n",
    "            state = np.random.choice(policy[state])\n",
    "        moves += 1\n",
    "\n",
    "        seq_states.append(state)\n",
    "        #to avoid having verry long sequences, we set a threshold to 10000 moves\n",
    "        #We advise to rise this number according the value of N disks since the bigger is N, the longer are the sequences\n",
    "        if moves > 10000:\n",
    "            print(\"Maximal number of moves reached: \", moves)\n",
    "            break\n",
    "\n",
    "\n",
    "    if debug :\n",
    "        print(\"play\")\n",
    "        print(\"len(seq_states) : \", len(seq_states))\n",
    "        print(\"seq_states : \",seq_states)\n",
    "    return moves, number_asked,seq_states\n",
    "\n",
    "def play_average(policy, allowed, play_times=100, \n",
    "                 epsilon=0, debug=False):\n",
    "\n",
    "    moves = np.zeros(play_times)\n",
    "    seq_states = {}\n",
    "    number_asked = np.zeros(play_times)\n",
    "    if debug :\n",
    "        print(\"play_average\")\n",
    "        print(\"allowed : \", allowed)\n",
    "        print(\"moves : \", moves)\n",
    "\n",
    "    for n in range(play_times):\n",
    "        if debug : print(\"\\n PLAY_TIME n° : \", n)\n",
    "        last_move = np.mean(moves)                \n",
    "        moves[n], number_asked[n],seq_states[n] = play(policy, allowed,epsilon, last_move)\n",
    "\n",
    "    if debug : print(\"moves with update : \", moves)\n",
    "    return np.mean(moves), np.std(moves), np.mean(number_asked), np.std(number_asked), seq_states\n",
    "\n",
    "def Q_performance(R, episodes, play_times=100, \n",
    "                  random=False, epsilon=0,debug=False):\n",
    "    if debug : print(\"\\n\\n Q_performance-------------------------------------\")\n",
    "    means = np.zeros(len(episodes))\n",
    "    seq_states = {}\n",
    "    stds = np.zeros(len(episodes))\n",
    "    asked = np.zeros(len(episodes))\n",
    "    asked_stds = np.zeros(len(episodes))\n",
    "    for n, N_episodes in enumerate(episodes):\n",
    "        if debug: print(\" ==> Training session :\", n, \" - N_episodes : \", N_episodes)\n",
    "        Q = learn_Q(R, N_episodes = N_episodes, random=random)\n",
    "        policy, best_policy_value = get_policy(Q,R)\n",
    "        if debug:\n",
    "            print(\"n :\", n, \" - N_episodes : \", N_episodes)\n",
    "            print(\"policy : \", policy, \" - best_policy_value : \", best_policy_value)\n",
    "        if n == 0:\n",
    "            allowed, best_policy_value = get_policy(Q,R)\n",
    "        means[n], stds[n], asked[n], asked_stds[n],seq_states[n] = play_average(policy, allowed, play_times, epsilon, debug)\n",
    "\n",
    "\n",
    "    return means, stds, asked, asked_stds,seq_states\n",
    "\n",
    "#Calculate the average Q learning performance\n",
    "def Q_performance_average(R, episodes, learn_times = 100, play_times=100, \n",
    "                          random=False, \n",
    "                          epsilon=0,\n",
    "                          debug=False):\n",
    "    if debug : print(\"Q_performance_average\")\n",
    "    means_times = np.zeros((learn_times, len(episodes)))\n",
    "    seq_states = {}\n",
    "    stds_times = np.zeros((learn_times, len(episodes)))\n",
    "    asked_times = np.zeros((learn_times, len(episodes)))\n",
    "    asked_stds_times = np.zeros((learn_times, len(episodes)))\n",
    "    for n in range(learn_times):\n",
    "        print(\" learn time : \", n)                                                     \n",
    "        means_times[n,:], stds_times[n,:], asked_times[n,:], asked_stds_times[n,:], seq_states[n]  = Q_performance(R, episodes, play_times=play_times,\n",
    "                                                                                                                   random=random, \n",
    "                                                                                                                   epsilon=epsilon) \n",
    "    means_averaged = np.mean(means_times, axis = 0)\n",
    "    stds_averaged = np.mean(stds_times, axis = 0)\n",
    "    asked_averaged = np.mean(asked_times, axis = 0)\n",
    "    asked_stds_averaged = np.mean(asked_stds_times, axis = 0)\n",
    "    return means_averaged, stds_averaged,seq_states\n",
    "\n",
    "\n",
    "\n",
    "def plot_Qlearning_average_number_of_moves(N, episodes,means_averaged,stds_averaged, result_dir):\n",
    "    fig = plt.figure()\n",
    "    optimum_moves = 2 ** N - 1\n",
    "    plt.plot(episodes, means_averaged, 'b-', linewidth=2, label='Average Performance')\n",
    "    plt.plot(episodes, means_averaged + stds_averaged, 'b-', alpha=0.5)\n",
    "    plt.plot(episodes, means_averaged - stds_averaged, 'b-', alpha=0.5)\n",
    "    plt.fill_between(episodes, means_averaged - stds_averaged, means_averaged + stds_averaged, facecolor='blue',\n",
    "                     alpha=0.5)\n",
    "    plt.axhline(y=optimum_moves, color='g', label='Optimum')\n",
    "    plt.xlabel('Number of training episodes')\n",
    "    plt.ylabel('Number of moves')\n",
    "    plt.grid('on', which='both')\n",
    "    plt.title('Q-learning - Towers of Hanoi game with %s discs' % N)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, fontsize=10)\n",
    "    res_file = result_dir+ \"/TOH_\"+str(N)+\"_disks_average_number_of_moves.png\"\n",
    "    plt.savefig(res_file)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Prepare the final plot\n",
    "\n",
    "def prepare_plot(episodes, N):\n",
    "    fig = plt.figure()\n",
    "    optimum_moves = 2**N - 1\n",
    "    plt.axhline(y=optimum_moves, color='g', label='Optimum (=%s moves)' % optimum_moves)\n",
    "    plt.xlabel('Number of training episodes')\n",
    "    plt.ylabel('Number of moves')\n",
    "    plt.grid('on', which='both')\n",
    "    plt.title('Q-learning of the Towers of Hanoi game with %s discs' % N)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels)\n",
    "\n",
    "def prepare_plot_aksed(episodes, N):\n",
    "    fig = plt.figure()\n",
    "    optimum_moves = 2**N - 1\n",
    "    plt.axhline(y=optimum_moves, color='g', label='Optimum (=%s moves)' % optimum_moves)\n",
    "    plt.xlabel('Number of training episodes')\n",
    "    plt.ylabel('Number of asked for help')\n",
    "    plt.grid('on', which='both')\n",
    "    plt.title('Q-learning of the Towers of Hanoi game with %s discs' % N)\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels)\n",
    "\n",
    "#Plot mean results\n",
    "def add_mean_results(episodes, means_averaged, color, label):\n",
    "    plt.loglog(episodes, means_averaged, color, label)\n",
    "\n",
    "#Plot mean results\n",
    "def add_ask_results(episodes, asked_averaged, color, label):\n",
    "    plt.loglog(episodes, means_averaged, color, label)\n",
    "\n",
    "\n",
    "#Plot standard deviations\n",
    "def add_stds_results(episodes, means_averaged, stds_averaged, color, facecolor, label):\n",
    "    plt.loglog(episodes, means_averaged + stds_averaged, color, alpha=0.5)\n",
    "    plt.loglog(episodes, means_averaged - stds_averaged, color, alpha=0.5)\n",
    "    plt.fill_between(episodes, means_averaged-stds_averaged, means_averaged+stds_averaged, facecolor=facecolor, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAIN CODE to Launch step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 3 #Number of disks in TOH\n",
    "if not path.exists(\"Results/\"):\n",
    "    os.mkdir(\"Results/\")\n",
    "    \n",
    "print(\"\\n\\n *********************************************************************** \")\n",
    "print(\" TOH with \", N, \" disks \")\n",
    "print(\"****************************************************************************\\n\\n\")\n",
    "\n",
    "result_dir_for_N_disks= \"Results/\"+str(N)+\"_disks_Results\"\n",
    "if not path.exists(result_dir_for_N_disks):\n",
    "    os.mkdir(result_dir_for_N_disks)\n",
    "\n",
    "\n",
    "print(\"\\n\\n-----------------------------------------------------------------------------------------------\")\n",
    "print(\"STEP 0 - TOH :  Define game variables and rules\")\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "R, R_values, states, moves = generate_reward_matrix(N)\n",
    "\n",
    "debug = False\n",
    "if debug :\n",
    "    print(\"R.shape : \", R_values.shape)\n",
    "    print(\"R : \", R_values)\n",
    "\n",
    "G = generate_graph_from_reward_matrix(N,R,result_dir_for_N_disks)\n",
    "toh_dict_grammar = generate_grammar_from_df(N, R, G,result_dir_for_N_disks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R.shape\n",
    "R.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Advised paramters from the article - Attention to computationnal time and memory consumption\n",
    "#Parameters for N=3\n",
    "episodes = [0, 100,300,500,1000]\n",
    "\n",
    "#Parameters for N=4\n",
    "#episodes = [0, 1000,2000,3000,4000]\n",
    "\n",
    "#Parameters for N=6\n",
    "#episodes = [0, 30000,60000,100000,150000]\n",
    "\n",
    "learn_times = 100\n",
    "play_times=100\n",
    "\n",
    "#Simple parameters to launch and execute quickly the code\n",
    "#episodes = [0, 1, 2, 5, 10]\n",
    "#learn_times = 10\n",
    "#play_times=10\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n-----------------------------------------------------------------------------------------------\")\n",
    "print(\"STEP 1 - QLEARNING \")\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "print(\"\\nSTEP 1.a - QLEARNING :  AA LEARNING, PLAYING and being recorded------------------------------- \\n\")\n",
    "\n",
    "print(\"List of training episodes : \", episodes)\n",
    "print(\" - Learning times : \", learn_times)\n",
    "print(\" - Play times : \", play_times)\n",
    "\n",
    "means_averaged, stds_averaged ,seq_states= Q_performance_average(R_values, episodes,learn_times, play_times, debug=False)\n",
    "plot_Qlearning_average_number_of_moves(N, episodes,means_averaged,stds_averaged, result_dir_for_N_disks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dict={}\n",
    "dict_sequences_of_moves={}\n",
    "nb_sequences=0\n",
    "\n",
    "print(\"\\nSTEP 1.b - QLEARNING :  extraction of sequences of moves to explicit the behavioral patterns------------------------------- \\n\")\n",
    "\n",
    "dict_sequences_of_moves_per_training_session={}\n",
    "for j, N_episodes in enumerate(episodes):\n",
    "    dict_sequences_of_moves_per_training_session[j]={'nb_training_episodes': N_episodes, 'sequences':[]}\n",
    "\n",
    "result_dir_Q_learning_step = result_dir_for_N_disks + \"/Step1-Q-learning/\"\n",
    "if not path.exists(result_dir_Q_learning_step):\n",
    "    os.mkdir(result_dir_Q_learning_step)\n",
    "\n",
    "for n in range(learn_times):\n",
    "    list_sequences_per_learning_time=[]\n",
    "    if debug : print(\"\\nlearn_times  n :\", n)\n",
    "    sequences_of_learn_times = seq_states[n]\n",
    "    dict_session={}\n",
    "    for j, N_episodes in enumerate(episodes):\n",
    "        list_sequences_per_learning_time_per_episodes=[]\n",
    "        if debug : print(\"\\n ==> j :\", j, \" - N_episodes : \",N_episodes)\n",
    "        list_sequences_of_episode = sequences_of_learn_times[j]\n",
    "        if debug : print(\"==> sequences_of_episode :\", list_sequences_of_episode)\n",
    "        dict_play_time={}\n",
    "\n",
    "        for i in range(play_times):\n",
    "\n",
    "            sequence_of_one_play_time= list_sequences_of_episode[i]\n",
    "            if debug :\n",
    "                print(\"\\n        play_times i :\", i)\n",
    "                print(\"        sequence_of_one_play_time : \", len(sequence_of_one_play_time), \" - \", sequence_of_one_play_time)\n",
    "            sequence_of_states = []\n",
    "            sequence_of_moves=[]\n",
    "\n",
    "            for z in range(len(sequence_of_one_play_time) - 1):\n",
    "                a_state=sequence_of_one_play_time[z]\n",
    "                sequence_of_states.append(states[a_state])\n",
    "                if debug : print(\"type(states[a_state]) : \",type(states[a_state]))\n",
    "\n",
    "                current_state = states[a_state]\n",
    "                next_state = states[sequence_of_one_play_time[z + 1]]\n",
    "\n",
    "                diff =\"\"\n",
    "                for t in range(len(current_state)):\n",
    "                    if current_state[t]!=next_state[t]:\n",
    "                        diff=str(current_state[t])+\"-\"+str(next_state[t])\n",
    "\n",
    "                if debug : print(\"current_state: \",current_state, \" - next_state : \", next_state, \" - diff : \", diff)\n",
    "                if diff !=\"\" :\n",
    "                    sequence_of_moves.append(diff)\n",
    "\n",
    "                if (z+1)==(len(sequence_of_one_play_time) - 1) :\n",
    "                    sequence_of_states.append(next_state)\n",
    "\n",
    "            if debug :\n",
    "                print(\"        sequence_of_states : \", len(sequence_of_states) , \" - \",  sequence_of_states)\n",
    "                print(\"        sequence_of_moves : \", len(sequence_of_moves), \" - \", sequence_of_moves)\n",
    "\n",
    "            dict_play_time[i] = {'id_play_time': i, 'sequence_of_move':sequence_of_moves, 'sequence_of_states':sequence_of_states}\n",
    "            list_sequences_per_learning_time_per_episodes.append(sequence_of_moves)\n",
    "            nb_sequences+=1\n",
    "\n",
    "        dict_session[j]={'id_session': j, 'session_data':dict_play_time}\n",
    "\n",
    "\n",
    "\n",
    "        list_sequences_per_learning_time = list_sequences_per_learning_time_per_episodes + list_sequences_per_learning_time\n",
    "\n",
    "        #Update dict_sequences_of_moves_per_training_session with these sequences of this id_learn_time\n",
    "        previous_seqs = dict_sequences_of_moves_per_training_session[j]['sequences']\n",
    "        previous_seqs= previous_seqs+ list_sequences_per_learning_time_per_episodes\n",
    "        dict_sequences_of_moves_per_training_session[j]['sequences'] = previous_seqs\n",
    "\n",
    "    dict_sequences_of_moves[n] = {'id_learn_time': n, 'N_episodes': N_episodes, 'learn_times_data': dict_session}\n",
    "    if debug : print(\" j : \", j, \" - N_episodes : \", N_episodes, \" - len dict_sequences_of_moves_per_training_session[j]['sequences'] : \", len(dict_sequences_of_moves_per_training_session[j]['sequences']))\n",
    "\n",
    "print(\"Saving of a total of \", nb_sequences, \" sequences into \",result_dir_Q_learning_step,\"dict_sequences_of_moves_per_training_session.json...\")\n",
    "tools.save_dict_in_json(dict_sequences_of_moves_per_training_session, result_dir_Q_learning_step+\"dict_sequences_of_moves_per_training_session.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in sorted(dict_sequences_of_moves_per_training_session.keys()):\n",
    "    file_name = result_dir_Q_learning_step+\"/id_training_sessions_\" + str(\n",
    "        key) + \"_sequences_of_moves_\"+str(learn_times)+\"_learn_times_\"+str(play_times)+\"_play_times\"\n",
    "    print(\"==>key \", key,\" : for id_training_session:\", str(key), \", with Nb training episodes :  \",dict_sequences_of_moves_per_training_session[key][\"nb_training_episodes\"],\n",
    "          \" :  \", len(dict_sequences_of_moves_per_training_session[key][\"sequences\"]), \" sequences are saved in : \",\n",
    "          file_name)\n",
    "    np.save(file_name, np.asarray(dict_sequences_of_moves_per_training_session[key][\"sequences\"]))\n",
    "\n",
    "global_dict={\"learn_times\":learn_times, \"play_times\": play_times, \"N_disk\":N, \"episodes\":episodes, \"dict_sequences_of_moves_per_training_session\":dict_sequences_of_moves_per_training_session}\n",
    "tools.save_dict_in_json(global_dict,result_dir_Q_learning_step+\"/2022-04-21-global_dict_with_dict_sequences_of_moves_per_training_session.json\")\n",
    "print(\"RESULTS: Global_dict with \",str(N), \"disks saved in \",result_dir_Q_learning_step,\"/2022-04-21-global_dict_with_dict_sequences_of_moves_per_training_session.json\")\n",
    "\n",
    "global_dict={\"learn_times\":learn_times, \"play_times\": play_times, \"N_disk\":N, \"episodes\":episodes, \"dict_sequences_of_moves\":dict_sequences_of_moves}\n",
    "tools.save_dict_in_json(global_dict,result_dir_Q_learning_step+\"/2022-04-21-global_dict.json\")\n",
    "print(\"RESULTS: Global_dict with \",str(N), \"disks saved in \",result_dir_Q_learning_step,\"2022-04-21-global_dict.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2 - Train the LSTM RNN with TOH sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Moves Sequence Learning Phase consists in training an RNN with LSTM units (named from here as the LSTM model) with the recorded sequences of moves of the AA. The model needs to learn to predict the AA’s next move at time t based on the current and past ones. <br/>\n",
    "This step returns a dataset of recordings of hidden patterns (i.e, the activity vectors of the hidden layer generated by the network at each input). <br/>\n",
    "The trained LSTM model had encoded an implicit representation of the TOH rules due to the learned sequences. \n",
    "Let us note that the trained LSTM model is trained on sequences generated from the TOH grammar. <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tohgrammar import TohGrammar\n",
    "\n",
    "\n",
    "print(\"\\n\\n-----------------------------------------------------------------------------------------------\")\n",
    "print(\"STEP 2 - RNN-LSTM : Encoding implicit TOH rules representation through learning \")\n",
    "print(\"-----------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# NEXT STEP :\n",
    "# use toh_grammar to generate sequences to train the LSTM network\n",
    "result_dir_RNN_LSTM_step = result_dir_for_N_disks + \"/Step2-RNN_LSTM/\"\n",
    "if not path.exists(result_dir_RNN_LSTM_step):\n",
    "    os.mkdir(result_dir_RNN_LSTM_step)\n",
    "    \n",
    "debug= False\n",
    "transitions=[]\n",
    "\n",
    "if debug : print(\"toh_dict_grammar.keys() : \", toh_dict_grammar.keys())\n",
    "for k in sorted(toh_dict_grammar.keys(), key=int):\n",
    "    state = toh_dict_grammar[k][\"label\"]\n",
    "    transitions_tuples= toh_dict_grammar[k][\"transitions\"]\n",
    "    for tuple in transitions_tuples :\n",
    "        if tuple[0] not in transitions :\n",
    "            transitions.append(tuple[0])\n",
    "\n",
    "print (\"Number of transitions in hanoi grammar : \", len(transitions))\n",
    "print (transitions)\n",
    "\n",
    "\n",
    "if debug : print( \"toh_dict_grammar.keys() : \", toh_dict_grammar.keys())\n",
    "hanoi_grammar = TohGrammar(transitions, toh_dict_grammar)\n",
    "TOH_dataset = hanoi_grammar.create_sequences_hanoi(10)\n",
    "\n",
    "file_name = result_dir_RNN_LSTM_step+\"TOH_dataset_\" + str(len(TOH_dataset)) + \"_sequences_\"+ str(N) + \"_disks\"\n",
    "print(\"TOH dataset of :\", str(len(TOH_dataset)),  \" sequences with TOH of\", str(N),\", disks saved in : \", file_name, \".npy\")\n",
    "np.save(file_name, np.asarray(TOH_dataset))\n",
    "\n",
    "transition_dict = hanoi_grammar.get_symbols_dict()\n",
    "\n",
    "print (\"Number of transitions in TOH grammar : \", len(transition_dict))\n",
    "for k, v in transition_dict.items():\n",
    "    print(k, \" : \", v)\n",
    "\n",
    "print(\"\\n STEP 2.a STEP 2 - RNN-LSTM :Loading TOH grammar and generation of sequences --------------------------\\n\")\n",
    "\n",
    "parameters_json_file = \"rnn_lstm_model_parameters.json\"\n",
    "print(\"Loading parameters for the RNN-LSTM step from the following file : \", parameters_json_file)\n",
    "\n",
    "dict_of_rnn_lstm_model_parameters = tools.get_dict_from_json(parameters_json_file)\n",
    "\n",
    "nb_sequences = dict_of_rnn_lstm_model_parameters[\"nb_sequences\"]\n",
    "keras_model_rnn_backup_file = dict_of_rnn_lstm_model_parameters[\"keras_model_rnn_backup_file\"]\n",
    "\n",
    "nb_hidden_layer = dict_of_rnn_lstm_model_parameters[\"nb_hidden_layer\"]\n",
    "nb_LSTM_cell_per_hidden_layer = dict_of_rnn_lstm_model_parameters[\"nb_LSTM_cell_per_hidden_layer\"]\n",
    "epochs = dict_of_rnn_lstm_model_parameters[\"epochs\"]\n",
    "debug = dict_of_rnn_lstm_model_parameters[\"debug\"]\n",
    "\n",
    "# files --------------------------------------------\n",
    "history_losses_png_file = dict_of_rnn_lstm_model_parameters[\"history_losses_png_file\"]\n",
    "losses_by_epoch_png_file = dict_of_rnn_lstm_model_parameters[\"losses_by_epoch_png_file\"]\n",
    "accuracy_by_epoch_png_file = dict_of_rnn_lstm_model_parameters[\"accuracy_by_epoch_png_file\"]\n",
    "\n",
    "rnn_lstm_model_backup_file = dict_of_rnn_lstm_model_parameters[\"rnn_lstm_model_backup_file\"]\n",
    "\n",
    "#test_sequences_json_file = dict_of_rnn_lstm_model_parameters[\"test_sequences_json_file\"]\n",
    "#test_predictions_and_HS_json_file = dict_of_rnn_lstm_model_parameters[\"test_predictions_and_HS_json_file\"]\n",
    "#rnn_weights_file = dict_of_rnn_lstm_model_parameters[\"rnn_weights_file\"]\n",
    "#rnn_for_hidden_states_LSTM_layer_weights_file = dict_of_rnn_lstm_model_parameters[\"rnn_for_hidden_states_LSTM_layer_weights_file\"]\n",
    "\n",
    "#analysis_of_artificial_agent_behavior = dict_of_rnn_lstm_model_parameters[\"analysis_of_artificial_agent_behavior\"],\n",
    "#artificial_agent_behavior_data_set = dict_of_rnn_lstm_model_parameters[\"artificial_agent_behavior_data_set\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rnnlstmmodel import RnnLstmModel\n",
    "from datasetofsequences import DataSetOfSequences\n",
    "\n",
    "print(\"\\n STEP 2.b  - RNN-LSTM :Training, evaluation and test of the model --------------------------\\n\")\n",
    "rnn_lstm_model = RnnLstmModel(transition_dict,nb_LSTM_cell_per_hidden_layer,debug)\n",
    "\n",
    "TOH_dataset_of_sequences = DataSetOfSequences(TOH_dataset,transition_dict)\n",
    "\n",
    "#TRAINING of the LSTM model --------------------------\n",
    "input_train_reshape= TOH_dataset_of_sequences.get_input_train_reshape()\n",
    "expected_output_train_reshape= TOH_dataset_of_sequences.get_expected_output_train_reshape()\n",
    "input_train_val_reshape=TOH_dataset_of_sequences.get_input_train_val_reshape()\n",
    "expected_output_train_val_reshape=TOH_dataset_of_sequences.get_expected_output_train_val_reshape()\n",
    "\n",
    "if debug : \n",
    "    print (\"epochs : \", epochs,\" - input_train_reshape.shape : \",input_train_reshape.shape ,\" - expected_output_train_reshape.shape : \", expected_output_train_reshape.shape ,\" - input_train_val_reshape.shape : \", input_train_val_reshape.shape ,\" - expected_output_train_val_reshape.shape : \", expected_output_train_val_reshape.shape ,\" - keras_model_rnn_backup_file : \",keras_model_rnn_backup_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rnn_lstm_model.train_model(epochs,input_train_reshape,expected_output_train_reshape,input_train_val_reshape,expected_output_train_val_reshape,\n",
    "                           rnn_lstm_model_backup_file, history_losses_png_file, losses_by_epoch_png_file, accuracy_by_epoch_png_file, \n",
    "                           result_dir_RNN_LSTM_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_reshape= TOH_dataset_of_sequences.get_input_test_reshape()\n",
    "expected_output_test_reshape= TOH_dataset_of_sequences.get_expected_output_test_reshape()\n",
    "rnn_lstm_model.evaluate_model (input_test_reshape, expected_output_test_reshape,input_train_reshape ,expected_output_train_reshape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING the LSTM model ------------------------------\n",
    "RNN_LSTM_dict_of_extracted_patterns= TOH_dataset_of_sequences.get_dict_of_extracted_patterns()\n",
    "rnn_lstm_model.test_model(result_dir_RNN_LSTM_step,input_test_reshape,transition_dict,RNN_LSTM_dict_of_extracted_patterns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3 - eXplainable process to explain the  knowledge construction and evolution of Qlearning Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The XAI Phase consist in a post-hoc implicit rule extraction algorithm and a graph visualization technique that are applied to the dataset of recorded hidden patterns to extract graphs of AA behavior at different stages of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "print(\"\\n\\n-----------------------------------------------------------------------------------------------\")\n",
    "print(\"STEP 3 - XAI : Loading XAI parameters and extraction of THE HIDDEN REPRESENTATIONS from RNN LSTM USING sequences from AA playing\")\n",
    "print(\"---------------------------------------------------------------------------------------------------\")\n",
    "result_dir_xai_step=result_dir_for_N_disks+\"/Step3-XAI/\"\n",
    "if not path.exists(result_dir_xai_step):\n",
    "    os.mkdir(result_dir_xai_step)\n",
    "\n",
    "xai_parameters_file = \"xai_hanoi.json\"\n",
    "print(\"Loading parameters for the XAI step from the following file : \", xai_parameters_file)\n",
    "parameters_dict = tools.get_dict_from_json(xai_parameters_file)\n",
    "\n",
    "clustering_range_min = parameters_dict[\"clustering_range_min\"]\n",
    "clustering_range_max = parameters_dict[\"clustering_range_max\"]\n",
    "debug = parameters_dict[\"debug\"]\n",
    "nb_patterns = parameters_dict[\"nb_patterns\"] #number of patterns you want to extract and explain. If equals to 0, the code will analyze all the recorded patterns\n",
    "\n",
    "\n",
    "assert (clustering_range_max > clustering_range_min)\n",
    "\n",
    "print(\"Parameters for rule extraction algorithms are : \")\n",
    "print(\"clustering_range_min : \", clustering_range_min)\n",
    "print(\"clustering_range_max : \", clustering_range_max)\n",
    "print(\"nb_patterns : \", nb_patterns)\n",
    "\n",
    "start_time = time.monotonic()\n",
    "print(\"\\n\\n dict_sequences_of_moves_per_training_session---------\")\n",
    "for id_training_session,v in dict_sequences_of_moves_per_training_session.items():\n",
    "    nb_training_episodes= dict_sequences_of_moves_per_training_session[id_training_session][\"nb_training_episodes\"]\n",
    "    print(\"\\n Analysis of training sesion id : \", id_training_session)  # ,\" - v:  \", v)\n",
    "\n",
    "    result_dir_id_session = result_dir_xai_step+\"N_episode_\"+str(id_training_session)+\"/\"\n",
    "    if not path.exists(result_dir_id_session):\n",
    "        os.mkdir(result_dir_id_session)\n",
    "\n",
    "    print(\"\\n STEP 3.a - XAI : Transform sequences of AA behavior into samples --------------------------\\n\")\n",
    "\n",
    "    AA_sequences = dict_sequences_of_moves_per_training_session[id_training_session][\"sequences\"]\n",
    "\n",
    "    [AA_input_test, AA_expected_output_test, AA_dict_for_test] = tools.transform_sequences_into_data_for_rnn(transition_dict,\n",
    "                                                                                                    AA_sequences,\n",
    "                                                                                                    True, debug)\n",
    "    print(\"\\nTotal AA_input_test samples          : \", len(AA_input_test))\n",
    "    print(\"Total expected_output_test samples: \", len(AA_expected_output_test))\n",
    "\n",
    "    dict_of_extracted_patterns = tools.initialize_dict_of_extracted_patterns(AA_sequences, debug)\n",
    "    if debug :\n",
    "        print(\"dict_of_extracted_patterns : \", len(dict_of_extracted_patterns))\n",
    "        print(\"dict_of_extracted_patterns.keys() : \", len(dict_of_extracted_patterns.keys()))\n",
    "        print(\"dict_of_extracted_patterns : \", len(dict_of_extracted_patterns))\n",
    "        print(dict_of_extracted_patterns[0])\n",
    "\n",
    "    print(\"\\n STEP 3.b - XAI : Reshape data for Keras Model--------------------------\\n\")\n",
    "\n",
    "    AA_input_test = np.asarray(AA_input_test)\n",
    "    AA_expected_output_test = np.asarray(AA_expected_output_test)\n",
    "\n",
    "    AA_input_test_reshape = np.reshape(AA_input_test, (AA_input_test.shape[0], 1, AA_input_test.shape[1]))\n",
    "    AA_expected_output_test_reshape = np.reshape(AA_expected_output_test,\n",
    "                                              (AA_expected_output_test.shape[0], 1, AA_expected_output_test.shape[1]))\n",
    "\n",
    "    print(\"\\nReshape test data ...\")\n",
    "\n",
    "    if debug:\n",
    "        print(\"\\nAA_input_test_reshape.shape: \", AA_input_test_reshape.shape)\n",
    "        print(\"input_test_reshape: \", AA_input_test_reshape)\n",
    "        print(\"\\n---------------\")\n",
    "        print(\"\\nexpected_output_test_reshape.shape: \", AA_expected_output_test_reshape.shape)\n",
    "        print(\"expected_output_test_reshape: \", AA_expected_output_test_reshape)\n",
    "\n",
    "    print(\"\\n STEP 3.c - XAI : Evaluate training for RNN Keras Model for test dataset--------------------------\\n\")\n",
    "\n",
    "\n",
    "    rnn_model = rnn_lstm_model.get_rnn_model()\n",
    "    results = rnn_model.evaluate(AA_input_test_reshape, AA_expected_output_test_reshape, batch_size=1, verbose=1)\n",
    "    print(\"     RNN evaluation (test seq)      - Average [loss, accuracy] : \", results)\n",
    "\n",
    "    print(\"\\n STEP 3.d - XAI : RNN Keras Model predictions for test dataset--------------------------\\n\")\n",
    "\n",
    "    AA_input_test_reshape = tf.cast(AA_input_test_reshape, tf.float32)\n",
    "\n",
    "    model_predictions, hidden_pattern_h, states_c = rnn_model.predict(AA_input_test_reshape)\n",
    "\n",
    "    print(\"\\n     Nb predictions for test dataset: \", len(model_predictions))\n",
    "    \n",
    "    if debug: print(len(dict_of_extracted_patterns), \" - \",  len(model_predictions), \" - \", len(hidden_pattern_h))\n",
    "    \n",
    "    dict_of_extracted_patterns = tools.update_dict_of_extracted_patterns(dict_of_extracted_patterns, model_predictions, hidden_pattern_h,\n",
    "                                                                          result_dir_RNN_LSTM_step + \"/dict_of_extracted_patterns.json\",\n",
    "                                                                         transition_dict, debug)\n",
    "    \n",
    "    print(\"Après update dict_of_extracted_patterns : \", len(dict_of_extracted_patterns))\n",
    "    print(\"dict_of_extracted_patterns.keys() : \", len(dict_of_extracted_patterns.keys()))\n",
    "    print(\"dict_of_extracted_patterns : \", len(dict_of_extracted_patterns))\n",
    "    print(dict_of_extracted_patterns[0])\n",
    "    tools.save_dict_in_json(dict_of_extracted_patterns,result_dir_id_session+\"dict_of_extracted_patterns.json\" )\n",
    "    print(\"\\n STEP 3.e - XAI : Knowledge Extraction PROCESS through FSA--------------------------\\n\")\n",
    "\n",
    "    tools.knowledge_extraction_process(dict_of_extracted_patterns, clustering_range_min, clustering_range_max,\n",
    "                                       nb_patterns,result_dir_id_session )\n",
    "\n",
    "\n",
    "end_time = time.monotonic()\n",
    "print(\"Execution time for N : \", N,\" disks --> \", timedelta(seconds=end_time - start_time) ,\" seconds\")\n",
    "dict_computational_time.update({N:{\"execution_time\":(end_time - start_time), \"optimum_moves\":(2 ** N - 1)}})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
